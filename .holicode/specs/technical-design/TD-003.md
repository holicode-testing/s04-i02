# TD-003: Technology Stack Decisions

**Status:** draft  
**Created:** 2025-08-17  

## Executive Summary
This document outlines the proposed technology stack for the AI-driven Content Summarization service, chosen to support the Modular Monolith architectural pattern, enable rapid development, and leverage the expertise of the fast-paced team. The stack is designed for a lean local development environment with future compatibility for GCP deployment.

## Programming Languages & Frameworks
**Chosen Stack:** Node.js with TypeScript.
- **Rationale:**
    - **Team Expertise:** Aligns with the skills of an experienced, fast-paced team, enabling quick ramp-up and development velocity.
    - **Ecosystem Maturity:** Node.js has a rich ecosystem for API development, and TypeScript provides strong typing for improved maintainability and reduced runtime errors.
    - **Unified Language:** Allows for a single language across the backend and potentially future frontend components, simplifying context switching.
    - **Performance:** Node.js is well-suited for I/O-bound tasks like making API calls to LLM services.

**Alternatives Considered:**
- **Option A: Python (e.g., Flask/FastAPI)**
  - Pros: Excellent for AI/ML, vast data science libraries.
  - Cons: Requires a separate language context for the team if they are primarily JavaScript/TypeScript. Might introduce complexities for a unified modular monolith.
  - Rejected because: While strong for AI, the overall team velocity and unified stack for a modular monolith are prioritized for the MVP.

- **Option B: Go (e.g., Gin/Echo)**
  - Pros: Excellent performance for concurrent tasks, strong type safety.
  - Cons: Steeper learning curve for teams not familiar with Go, smaller ecosystem for web development compared to Node.js/Python.
  - Rejected because: Performance is not the primary bottleneck for the MVP, and team ramp-up time is critical.

**Trade-offs:**
- Node.js's single-threaded nature requires careful handling of CPU-bound tasks (though most LLM interactions are I/O-bound). Async operations are well-supported.

## Databases and Storage Solutions
**Chosen Approach:** Initial MVP will focus on in-memory or file-based storage for temporary data if persistence is not immediately required by a core feature. For any required persistence, a lightweight, embedded database like SQLite or a simple key-value store might be considered for local development. For future GCP deployment, Cloud SQL (PostgreSQL) or Firestore could be used.
- **Rationale:**
    - **Simplicity for MVP:** Avoids premature optimization and complex database setup for initial feature development.
    - **Flexibility:** Allows for easy pivoting on data storage solutions as requirements evolve.
    - **GCP Compatibility:** Cloud SQL and Firestore are managed services on GCP, offering scalability and reliability for future needs.

**Alternatives Considered:**
- **Option A: MongoDB/NoSQL**
  - Pros: Flexible schema, good for rapidly changing data models.
  - Cons: Can lead to schema ambiguity, eventual consistency challenges.
  - Rejected because: Relational (SQL) databases often provide clearer data models for business logic, and the MVP doesn't demand the specific benefits of NoSQL.

**Trade-offs:**
- In-memory storage means data loss on application restart, suitable only for non-critical or transient data. Introduction of a database will require additional setup and management.

## Message Queues and Event Streaming
**Chosen Approach:** Not required for the initial MVP of the summarization steel thread. If asynchronous processing or inter-module communication becomes necessary, a lightweight in-process event emitter or a simple message queue like Redis Pub/Sub could be considered for local development. For GCP, Cloud Pub/Sub would be the preferred solution.
- **Rationale:**
    - **Keep It Simple (KISS):** Avoids adding unnecessary complexity for the initial MVP.
    - **Scalability (Future):** Cloud Pub/Sub provides a highly scalable and reliable messaging service on GCP for future asynchronous needs.

**Alternatives Considered:**
- **Option A: Apache Kafka / RabbitMQ**
  - Pros: Robust, high-throughput, widely adopted for complex event streaming.
  - Cons: Significant operational overhead, complex setup for local development.
  - Rejected because: Over-engineering for MVP; adds undue complexity.

**Trade-offs:**
- Relying on direct function calls or simple event emitters limits true asynchronous processing and decoupling, but is acceptable for the initial scope.

## Caching Strategies
**Chosen Approach:** Not required for the initial MVP. If performance bottlenecks are identified due to repeated LLM calls, an in-memory cache (e.g., using `node-cache` or a simple Map) could be implemented for local development. For GCP, Cloud Memorystore (Redis) or a CDN (for static content, if any) could be used.
- **Rationale:**
    - **Optimize Later:** Focus on core functionality first.
    - **Targeted Optimization:** Implement caching only where actual performance gains are needed.

## Build and Development Tooling
- **Package Manager:** `pnpm` (or `npm`/`yarn` if already established in the project)
    - **Rationale:** Efficient disk space usage and faster installs due to content-addressable store.
- **Transpiler/Bundler:** `tsc` (TypeScript Compiler) for compilation. Webpack/Rollup/Vite may be introduced later if bundling for client-side or complex server-side optimization is needed.
- **Testing Framework:** `Jest` or `Vitest` (for unit and integration tests)
    - **Rationale:** Widely adopted, good performance, comprehensive features.
- **Code Quality:** `ESLint` and `Prettier`
    - **Rationale:** Enforce consistent code style and identify potential issues early.
- **Containerization:** `Docker` and `docker-compose`
    - **Rationale:** Provide consistent development environments and enable easy deployment to Cloud Run.

## LLM Integration
- **Approach:** Direct HTTP API calls to LLM providers (OpenAI, OpenRouter, LM Studio endpoints) using a well-structured client library or `fetch`/`axios`.
- **Rationale:**
    - **Flexibility:** Allows switching between providers easily.
    - **Control:** Provides full control over request/response parsing and error handling.
    - **Local Models:** Supports integration with local LLM servers (LM Studio, Ollama) via OpenAI API compatibility.

## Secrets Management
- **Local Development:** Environment variables (`.env` files) for local secrets.
- **Future GCP Deployment:** GCP Secret Manager for secure storage and access of API keys and other sensitive configurations.
